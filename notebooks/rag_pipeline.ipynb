{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8270581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load vector store and metadata\n",
    "index = faiss.read_index(\"../notebooks/vector_store/complaints_faiss.index\")\n",
    "with open(\"vector_store/metadata.pkl\", \"rb\") as f:\n",
    "    metadatas = pickle.load(f)\n",
    "\n",
    "# Load original data for source retrieval\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/processed/filtered_complaints.csv\")\n",
    "\n",
    "# Load embedding model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "def retrieve_chunks(question, k=5):\n",
    "    \"\"\"Embed question and retrieve top-k most similar chunks.\"\"\"\n",
    "    q_emb = embedder.encode([question], convert_to_numpy=True)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    retrieved = []\n",
    "    for idx in I[0]:\n",
    "        meta = metadatas[idx]\n",
    "        chunk_text = df.loc[meta[\"complaint_id\"], \"cleaned_narrative\"]\n",
    "        retrieved.append({\n",
    "            \"chunk\": chunk_text,\n",
    "            \"meta\": meta\n",
    "        })\n",
    "    return retrieved\n",
    "\n",
    "def build_prompt(question, retrieved_chunks):\n",
    "    \"\"\"Format the prompt for the LLM.\"\"\"\n",
    "    context = \"\\n\\n\".join([c[\"chunk\"] for c in retrieved_chunks])\n",
    "    prompt = (\n",
    "        \"You are a financial analyst assistant for CrediTrust. \"\n",
    "        \"Your task is to answer questions about customer complaints. \"\n",
    "        \"Use the following retrieved complaint excerpts to formulate your answer. \"\n",
    "        \"If the context doesn't contain the answer, state that you don't have enough information.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_answer(prompt, model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
    "    \"\"\"Generate answer using an LLM (HuggingFace pipeline example).\"\"\"\n",
    "    from transformers import pipeline\n",
    "    pipe = pipeline(\"text-generation\", model=model_name, max_new_tokens=256)\n",
    "    response = pipe(prompt)[0][\"generated_text\"]\n",
    "    # Optionally, extract only the answer part\n",
    "    return response.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "def rag_qa(question, k=5, llm_model=\"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
    "    retrieved = retrieve_chunks(question, k)\n",
    "    prompt = build_prompt(question, retrieved)\n",
    "    answer = generate_answer(prompt, model_name=llm_model)\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_sources\": retrieved[:2]  # Show 1-2 for report\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7effc039",
   "metadata": {},
   "source": [
    "## RAG Pipeline Evaluation\n",
    "\n",
    "| Question | Generated Answer | Retrieved Sources (excerpt) | Quality Score (1-5) | Comments/Analysis |\n",
    "|----------|-----------------|-----------------------------|---------------------|-------------------|\n",
    "| Why are customers unhappy with Buy Now, Pay Later? | ... | ... | 4 | Good summary, but missed some nuance. |\n",
    "| How often do complaints mention late fees? | ... | ... | 5 | Accurate and referenced context. |\n",
    "| Are there complaints about savings account closures? | ... | ... | 3 | Somewhat relevant, but context was thin. |\n",
    "| What issues do people report with money transfers? | ... | ... | 5 | Comprehensive and well-supported. |\n",
    "| Do customers mention fraud in personal loans? | ... | ... | 4 | Detected fraud mentions, but could be more specific. |\n",
    "\n",
    "**Analysis:**  \n",
    "The RAG pipeline generally retrieves relevant complaint excerpts and produces coherent, context-grounded answers. Performance is strongest for well-represented topics (e.g., late fees, money transfers). For less frequent or ambiguous queries, the system sometimes retrieves less relevant chunks, which can reduce answer quality. Prompt engineering and chunking strategy both contribute to overall effectiveness. Future improvements could include more advanced reranking, chunk filtering, or using a larger LLM for generation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
